<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>reveal.js - The HTML Presentation Framework</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>

    <!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>Mimic</h1>
          <h2>An API Compatible Mock Service For OpenStack</h2>
        </section>

        <section>
          <h1>Introductions</h1>
          <aside class="notes">
            <p>Lekha: Hi, I am Lekha Jeevan, a software developer in Test in Rackspace</p>
            <p>Glyph: and I'm Glyph, also a software developer at Rackspace.</p>
          </aside>
        </section>

        <section>
          <h1>What?</h1>
          <aside class="notes">
            Lekha: Today we are going to talk about Mimic - an open source
            framework that allows for testing against for OpenStack and
            Rackspace APIs, how it is making testing for products in Rackspace
            a cake walk and how it could potentially do the same for your
            OpenStack-backed applications. Your contributions, will help us get
            there.
          </aside>
        </section>

        <section>
          <h1>Why?</h1>
          <aside class="notes">
            Glyph: While Mimic is hopefully general purpose, it was originally
            created for testing a specific application: Rackspace Auto Scale.
            So let's talk about that first.
          </aside>
        </section>

        <section>
          <h1>Auto Scale</h1>
          <aside class="notes">
            Lekha: Rackspace Autoscale is a system which automates the process
            of getting the right amount of compute capacity for an application,
            by creating (scaling up) and deleting (scaling down) servers and
            associating them with load balancers.
          </aside>
        </section>

        <section>
          <h1>Dependencies</h1>
          <h2>(of Auto Scale)</h2>
          <aside class="notes">
            Lekha: In order to perform this task, Autoscale speaks to three
            back-end APIs: Rackspace Identity for authentication and
            impersonation, Rackspace Cloud Servers for provisioning and
            deleting servers, and Rackspace Cloud Load Balancers for adding and
            removing servers to load balancers as they are created and deleted.

            Rackspace Identity is API-compatible with OpenStack Identity v2,
            Rackspace Cloud Servers is powered by and API-compatible with
            OpenStack Compute, although Rackspace Cloud Load Balancers is a
            custom API.
          </aside>
        </section>

        <section>
          <h1>Testing</h1>
          <h2>(for Auto Scale)</h2>
          <aside class="notes">
            Lekha: The development started, and I was the QE in the group, and
            I started writing automated tests, in parallel to the development
            work. So, as autoscale was interacting with so many other systems,
            testing it dint mean just testing autoscale, but that if any of
            these systems it depended on didn't behave as expected, autoscale
            did not crumble. But was consistent and able to handle them.
          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>(for Auto Scale)</h2>
          <aside class="notes">
            Lekha: So, I had two kinds of tests, one was the functional tests,
            to test the API contracts, verify the responses of the API calls
            given valid requests, or malformed requests.

            The other, was system integration tests. These were more complex
            tests, as they were going to be verifying the integration between
            Identity(keystone), Compute, Load balancers and Autoscale. For
            example, When a user created a scaling group, it will verify that
            the minimum number of servers on the group are provisioned
            successfully, as in the servers are active and they exist as nodes
            on the load balancers. Or if a server, went into an error state,
            that can happen, Autoscale was able to re-provision that server.
          </aside>
        </section>

        <section>
          <h1>Testing Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">
            Lekha: All these tests were running against the real
            services. Servers could take over a minute, ten minutes to
            provision and the tests would run that much longer. Sometimes, the
            tests would fail due to random failures, like a server would go
            into error state, where as the test was expecting it to go into an
            active state. The tests for the negative scenarios, like actually
            testing how autoscale would behave if the server went into an error
            state, could not be tested. This is just not something that could
            be reproduced.
          </aside>
        </section>

        <section>
          <h1>More Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">
            Lekha: Well, the test coverage was improving as I continued to add
            tests, oblivious of the time it was taking run the entire test
            suite! Now, we had started using these tests as a gate in out merge
            pipeline. But the tests were running for so long and were sometimes
            flaky. Nobody dared to run these tests locally! Not even me, when I
            was adding more tests! Also, our peers from the compute and load
            balancers teams, whose resources we were using up for our
            "Auto-scale" testing, were _not_ happy! So much, so that, we were
            pretty glad, we were in a remote office!
          </aside>
        </section>

        <section>
          <h1>We've Had Enough!</h1>
          <h2>(on Auto Scale)</h2>
          <aside class="notes">
            Lekha: This had to change! we needed something! to save us from the
            slow and flaky testing!
          </aside>
        </section>

        <section>
          <h1>There And Back Again</h1>
          <h2 class="fragment current-visible">Specific → General</h2>
          <h2 class="fragment current-visible">General → Specific</h2>
          <aside class="notes">
            Glyph: Now that we've had enough, how are we going to solve this
            problem?  Since we've been proceeding from the specific case of
            Autoscale to the general utility of Mimic, (click) let's go back to
            the general and proceed to the specific.
          </aside>
        </section>

        <section>
          <h1>From The General</h1>
          <h2>Testing For Failure</h2>

          <aside class="notes">
            When you have a complex distributed system - that is, a service A
            which makes requests of another service B - you have to deal with
            the whole spectrum of real failures which might come back from
            service B.
          </aside>
        </section>
        <section>
          <h1>Real Services Fail</h1>
          <aside class="notes">
            Testing against the real service is of course important for
            validating a product, ensuring that it works as expected in a
            realistic deployment environment, even if your colleagues on the
            compute and load balancer teams get mad at you.
          </aside>
        </section>

        <section>
          <h1>(Unpredictably)</h1>
          <aside class="notes">
            But when you write code to interact with a service, you need to
            handle a wide variety of error cases.  Those errors won't come back
            from the real service every time.
          </aside>
        </section>

        <section>
          <h1>Succeeding</h1>
          <h2>At Success</h2>
          <aside class="notes">
            Your positive-path code - the code that submits a request and gets
            the response that it expects - is going to get lots of testing in
            the real world.  Most interactions with services are successful,
            and the operators of those services always strive to make ever more
            of those interactions successful.  So most likely, the
            positive-path code is going to get exercised all the time and you
            will have plenty of opportunities to flush out bugs.
          </aside>
        </section>
        <section>
          <h1>Means Failing</h1>
          <h2>At Failure</h2>
          <aside class="notes">
            Your negative-path code, on the other hand, will only get invoked
            in production when there's a real error.  If everything is going as
            planned, this should be infrequent.
          </aside>
        </section>
        <section>
          <h1>Failure</h1>
          <aside class="notes">
            It's really important to get negative-path code right.  If
            everything is going well, it's probably okay if your code has a
            couple of bugs.  You might be able to manually work around them.
          </aside>
        </section>
        <section>
          <aside class="notes">
            But if things are starting to fail with some regularity in your
            cloud, that is exactly the time you want to make sure *your* system
            is behaving correctly: accurately reporting the errors, measuring
            the statistics, and allowing you to stay on top of incident
            management for your service.
          </aside>
        </section>
        <section>
          <aside class="notes">
            When you test against a real service, you are probably testing
            against a staging instance.  And, if your staging instance is
            typical, it probably doesn't have as much hardware, or as many
            concurrent users, as your production environment.  Every additional
            piece of harware or concurrent user is another opportunity for
            failure, so that means your staging environment is probably even
            less likely to fail.
          </aside>
        </section>
        <section>
          <aside class="notes">
            I've been in the software industry for long enough now to remember
            where this part of the talk would be the hardest part - the part
            where we try to sell the idea that code coverage and automated
            testing is really important.  Luckily we have moved on from the bad
            old days of the 90s, when most teams didn't have build automation,
            and if they wanted it they might not even be able to afford it.
          </aside>
        </section>
        <section>
          <aside class="notes">
            Luckily today we are all somewhat more enlightened, and we know
            that testing is important and full code coverage is important.  So
            when we write code like this:
          </aside>
        </section>
        <section>
          <aside class="notes">
            (slide: try/except with code)

            ... we know that we need to write tests for this part:

            (highlight 'except' part)

            ... by writing a mock for it.
          </aside>
        </section>
        <section>
          <aside class="notes">
            The problem with the traditional definition of a mock is that each
            mock is defined just for the specific test that it's mocking.
          </aside>
        </section>
        <section>
          <aside class="notes">
            Let's take a specific example from OpenStack Compute.
          </aside>
        </section>
        <section>
          <aside class="notes">
            In June of this year, OpenStack Compute [introduced a
            bug](https://github.com/openstack/nova/commit/1a6d32b9690b4bff709dc83bcf4c2d3a65fd7c3e)
            making it impossible to revoke a certificate.

            (This is not a criticism of Nova itself; [the bug was later
            fixed](https://github.com/openstack/nova/commit/96b39341d5a6ea91d825d979e2381b9949b26e27).)

            The bug here is that `chdir` does not actually return a value.
            Because these tests construct their own mocks for `chdir`, we
            properly cover all the code, but the code is not integrated with a
            system that is verified in any way against what the real system (in
            this case, Python's chdir) does.

            In this *specific* case, we might have simply tested against a real
            directory structure in the file system, because relative to the
            value of testing against a real implementation, creating a
            directory is not a terribly expensive operation.

            However, an OpenStack cloud is a significantly more complex system
            than `chdir`.  If you are developing an application that depends on
            OpenStack, creating a real cloud to test against is far too
            expensive and slow as Autoscale's experience shows.  Creating a
            one-off mock for every test is fast to get started with and fast to
            run, but is error prone and rapidily becomes a significant
            maintainance burden of its own right. Autoscale needed something
            that was quick to deploy, and lightweight to run like a mock, but
            realistic and usable in an integration scenario like a __real__
            system.

            This is were Mimic comes in.
          </aside>
        </section>

        <section>
          <h1>First Version of Mimic</h1>
          <aside class="notes">
            Mimic was built as a stand-in service for Identity, Compute and
            Rackspace Load balancers, the services that autoscale depends
            upon. Mimic initially implemented only the subset of each of these
            endpoints needed by Autoscale.
          </aside>
        </section>
        <section>
          <aside class="notes">
            The essence of Mimic is pretending.  The first thing that you must
            do to interact with it is to pretend to authenticate.
          </aside>
        </section>
        <section>
          <aside class="notes">
            Although Mimic does not validate credentials - all authentications
            will succeed - as with the real Identity endpoint, Mimic's identity
            endpoint has a service catalog which includes URLs for all the
            services it provides.  A well behaved OpenStack client will use the
            service catalog to look up URLs for its service endpoints. Such a
            client will only need two pieces of configuration to begin
            communicating with the cloud, i.e. credentials and the identity
            endpoint. A client written this way will only need to change the
            Identity endpoint to be that of Mimic.
          </aside>
        </section>
        <section>
          <aside class="notes">
            When you ask Mimic to create a server, it pretends to create
            one. This is not like stubbing with static responses: when Mimic
            pretends to build a server, it remembers the information about that
            server and will tell you about it in the subsequent requests.
          </aside>
        </section>
        <section>
          <aside class="notes">
            Mimic was originally created to speed things up. So, it was very
            important that it be fast both to respond to requests and to have
            developers setup. So it was built using in-memory data
            structures. Mimic has minimal software dependencies - almost
            entirely pure Python. No service dependencies and no
            configuration. It is entirely self- contained.
          </aside>
        </section>
        <section>
          <aside class="notes">
            Here is a quick demo.
          </aside>
        </section>

        <section data-state="first-demo">
          <video src="bootstrap.mp4" id="first-demo-video"></video>
          <aside class="notes">
            As you can see here, it is only a 3 step process to get mimic
            started, and be able to hit the APIs it implements.  The steps are
            pip install mimic, run mimic and hit the endpoint! That's it.  As
            we mentioned, the way you hit the endpoint is to simply change your
            client's Identity URL to Mimic's and run your code as before.
          </aside>
        </section>

        <section>
          <aside class="notes">
            (Demo here: nova python client pretending to create a server, list
            servers, delete server, against mimic.)

            Here is an example of how we could use Mimic against the OpenStack
            nova python client.  We're including the Mimic identity endpoint as
            the `AUTH_URL`.
          </aside>
        </section>

        <section>
          <h1>Using Mimic</h1>
          <h2>(on Auto Scale)</h2>
          <h3>TODO: split up this slide</h3>

          <aside class="notes">
            We did this with Autoscale, and pointed its tests at a Mimic
            instance.  This reduced the test time exponentially. Before Mimic,
            the functional tests would take 15 minutes, and now they run in 30
            seconds. The system integration tests would take 3 *hours* or more,
            cause if one of the servers in the test remained in the "building"
            state for fifteen minutes longer than usual, then the tests ran
            fifteen minutes slower.  That 3 *hours* (or more) went down to be
            less than 3 *minutes* (consistently) to complete!
          </aside>
        </section>
        <section>
          <aside class="notes">

            Our dev VMs are configured to run tests against Mimic, so
            developers get immediate feedback on the code being written and
            they can work offline without having to worry about uptimes of the
            upstream systems.
          </aside>
        </section>
        <section>
          <aside class="notes">

            Glyph: But Lekha, what about all the error injection stuff I
            mentioned before?  Does Mimic do that?  How did this set-up test
            Autoscale's error conditions?
          </aside>
        </section>
        <section>
          <aside class="notes">

            Well Glyph, I am as pleased as I am suprised that you ask that.
            Mimic does simulate errors.
          </aside>
        </section>
        <section>
          <aside class="notes">

            Earlier, when I said Mimic pretends to create a server, that wasn't
            entirely true - sometimes Mimic pretends to *not* create a server.
            It uses the metadata provided during the creation of the server. It
            inspects the metadata, and sets the state of the server
            respectively. For example, setting a `metadata` of say
            `"server_building": 30`, will make the server stay in building
            state for 30 seconds. Similarly, servers can be made to go into
            error state on creation.
          </aside>
        </section>
        <section>
          <aside class="notes">

            Autoscale's purpose is to get the user the right number of servers,
            even if a number of attempts to create one failed. We chose to use
            `metadata` for error injection so that requests with injected
            errors could also be run against real services. For Autoscale, the
            expected end result is the same number of servers created,
            irrespective of the number of failures. But this behavior may also
            be useful to many other applications because retrying is a common
            pattern for handling errors.
          </aside>
        </section>
        <section>
          <aside class="notes">

            However, the first implementation of mimic had some flaws, it was
            fairly Rackspace specific. It implemented only the services
            required by autoscale, and they were all implemented as the
            core. It ran each service on a different port, meaning that for N
            endpoints you would need not just N port numbers, but N
            *consecutive* port numbers. It allowed for testing error scenarios,
            but only using the metadata. This was not useful for all cases, for
            example, for a control panel that does not not allow every UI
            action to contain metadata.

            (TODO: ask someone if Horizon matches this description?)
          </aside>
        </section>
        <section>
          <aside class="notes">
            Mimic also did not implement multiple regions.  It used global
            variables for storing all state, which meant that it was hard to
            add additional endpoints with different state in the same running
            mimic instance.
          </aside>
        </section>

        <section>
          <h1>Refactoring Mimic</h1>
          <h2>(To Use with things other than Auto Scale)</h2>
          <aside class="notes">
            Mimic had an ambitious vision: to be a one-stop mock for all
            OpenStack and Rackspace services that needed fast integration
            testing.  However, its architecture at the time severely limited
            the ability of other teams to use it or contribute to it.  As Lekha
            mentioned, it was specific not only to Rackspace but to Autoscale.

          </aside>
        </section>
        <section>
          <aside class="notes">

            On balance, Mimic was also extremely simple.  It followed the You
            Aren't Gonna Need It principle of extreme programming very well,
            and implemented just the bare minimum to satisfy its requirements,
            so there wasn't a whole lot of terrible code to throw out or much
            unnecessary complexity to eliminate.
          </aside>
        </section>
        <section>
          <aside class="notes">

            There is, however, a corrolary to YAGNI, which is E(ITO)YAGNI:
            Eventually, It Turns Out, You *Are* Going To Need It.  As Mimic
            grew, other services within Rackspace wanted to make use of its
            functionality, and a couple of JSON response dictionaries in global
            variables were not going to cut it any more.
          </aside>
        </section>
        <section>
          <aside class="notes">

            So we created a plugin API.
          </aside>
        </section>
        <section>
          <aside class="notes">

            As Lekha mentioned previously, Mimic's Identity endpoint was the
            top-level entry point to Mimic as a service; every other URL was
            available from within the service catalog.  As we were designing
            the plugin API, it was clear that this top-level Identity endpoint
            needed to be the core part of Mimic, and plug-ins would each add an
            entry for themselves to the service catalog.
          </aside>
        </section>
        <section>
          <aside class="notes">

            Each plugin implements the `IAPIMock` interface, which has only two
            methods: `catalog_entries` and `resource_for_region`.

            `catalog_entries` takes a tenant ID and returns an iterable of
            `Entry` objects, each of which is a name and a collection of
            `Endpoint` objects, each containing a region, a URI version prefix,
            and a tenant ID of its own.

            Put more simply, APIs have catalog entries for each API type, which
            in turn have endpoints for each virtual region they represent.
          </aside>
        </section>
        <section>
          <aside class="notes">

            `resource_for_region` takes the name of a region, a URI prefix -
            produced by Mimic core to make URI for each service unique - and a
            session store where the API mock may look up state of the resources
            it pretended to provision for the respective
            tenants. `resource_for_region` returns an HTTP resource associated
            with the top level of the given region.  This resource then routes
            this request to any tenant- specific resources associated with the
            full URL path.

          </aside>
        </section>
        <section>
          <aside class="notes">
            Mimic uses the Twisted plugin system, which has a very simple model
            of how plugins work: you have an abstract interface - some methods
            and attributes that you expect of a plugin - and then plugins
            register themselves by instantiating a provider of that interface
            and placing that instance into a module within a particular
            namespace package.  In Mimic's case, that interface is `IAPIMock`
            and that package is `mimic.plugins`.

          </aside>
        </section>
        <section>
          <aside class="notes">
            Each service can contribute to the service catalog, provide entries
            and endpoints, and each endpoint gets added within the URI
            hierarchy.

            <!-- TODO: Demo of implementing a plugin! -->

          </aside>
        </section>
        <!-- TODO: insert a segue or transition here! -->
        <section>
          <h1>Error Conditions Repository</h1>
          <aside class="notes">
            Anyone testing a product, will run into unexpected errors. Thats
            why we test!  But we dont know what we dont know, and cant be
            prepared for this ahead of time right!
          </aside>
        </section>
        <section>
          <aside class="notes">

            When we were running the autoscale tests against Compute, we began
            to see some one-off errors. Like, when provisioning a server, the
            test expected a server to go into a building state for some time
            before it is active, __but__ it would remain in building state for
            over an hour or even would sometimes go into an error state.
          </aside>
        </section>
        <section>
          <aside class="notes">

            Autoscale had to handle such scenarios gracefully and the code was
            changed to do so. And Mimic provided for a way to tests this
            consistently.
          </aside>
        </section>
        <section>
          <aside class="notes">

            However, like I said, we dont know what we dont know. We were not
            anticipating any other such errors. But, there were more! And this
            was slow process for us to uncover such errors as we tested against
            the real systems. And we continued to add them to Mimic.
          </aside>
        </section>
        <section>
          <aside class="notes">

            Now, Wont it be great if not every person writing an application
            that used compute as a dependency had to go through this same cycle
            and have to find all the possible error conditions in a system by
            experience and have to deal with them at the pace that they occur.
          </aside>
        </section>
        <section>
          <aside class="notes">

            What if we had a repository of all such known errors, that everyone
            contributes to. So the next person using the plugin can use the
            existing ones, and ensure there application behaves consistently
            irrespective of the errors, and be able add any new ones to it.
          </aside>
        </section>
        <section>
          <aside class="notes">

            Mimic is just that, a repository of all known responses including
            the error responses.
          </aside>
        </section>

        <section>
          <aside class="notes">
            control of time & error injection
          </aside>
        </section>

        <section>
          <h1>Call to Action</h1>
          <aside class="notes">
            Mimic, can be the tool, where you do not have to stand up the
            entire dev stack to understand how an OpenStack API behaves.

            Mimic can be the tool which enables an OpenStack developer to get
            quick feedback on the code he/she is writing and not have to go
            through the gate multiple times to understand that, maybe I should
            have handled that one error, that the upstream system decides to
            throw my way every now and then.
          </aside>
        </section>

        <!--

            <section>
              <aside class="notes">
              </aside>
            </section>

         -->
      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>

    <script src="configuration.js"> </script>
    <script src="custom.js"></script>

  </body>
</html>
