<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>reveal.js - The HTML Presentation Framework</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>

    <!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>Mimic</h1>
          <h2>An API Compatible Mock Service For OpenStack</h2>
        </section>

        <section>
          <h1>Lekha</h1>
          <h3 style="color: gray"> software developer in test</h3>
          <h3 style="color: gray"> (at) Rackspace </h3>
          <aside class="notes">
            <p>Lekha: Hi, I am Lekha Jeevan, a software developer in test at Rackspace</p>
          </aside>
        </section>

        <section>
          <h1>Glyph</h1>
          <h3 style="color: gray"> software developer </h3>
          <h3 style="color: gray"> (at) Rackspace </h3>
          <aside class="notes">
            <p>Glyph: and I'm Glyph, also a software developer at Rackspace.</p>
          </aside>
        </section>

        <section>
          <h1>Mimic</h1>
          <aside class="notes">

            Lekha: Today Glyph and I are going to be talking to you all about
            Mimic. An opensource framework that allows for testing of
            OpenStack and Rackpsace APIs.

          </aside>
        </section>

        <section>
          <h1>How?</h1>
          <aside class="notes">

           Lekha: How does Mimic do that you ask.

          </aside>
        </section>

        <section>
          <h1>Mimic</h1>
          <h1>PRETENDS!</h1>
          <aside class="notes">

           Lekha: well, it just pretends to be the OpenStack or Rackspace API! 

          </aside>
        </section>

        <section>
          <h1>Sneak Peek</h1>
          <aside class="notes">

           Lekha: Before we even begin to dive into the details, let us take a
           quick sneak peek of how __easy__ it is to get started with Mimic.

          </aside>
        </section>

        <section data-state="first-demo">
          <video src="bootstrap.mp4" id="first-demo-video"></video>
          <aside class="notes">

            Its only a 3 step process: pip install mimic, run mimic and hit
            the endpoint! And... there is our endpoint to be able to
            Authenticate and get a service catalog containing the (OpenStack) services
            Mimic implements.

          </aside>
        </section>

        <section>
          <h1>Why?</h1>
          <aside class="notes">

           Lekha: Now, why does one need this you might wonder.

          </aside>
        </section>

        <section>
          <h1>Rewind</h1>
          <!--img class="transparent" src="rewind.png" style="border:5px gray" height="600"-->
          <aside class="notes">

           Lekha: Lets go back and see how Rackspace AutoScale was doing without Mimic.

          </aside>
        </section>

        <section>
          <h1>Testing</h1>
          <h2>(for Auto Scale)</h2>
          <aside class="notes">
            The development started, and I was the QE in the group, and I
            started writing automated tests, in parallel to the development
            work. So, as autoscale was interacting with so many other systems,
            testing it dint mean just testing autoscale, but that if any of
            these systems it depended on didn't behave as expected, autoscale
            did not crumble. But was consistent and able to handle them.
          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>(for Auto Scale)</h2>
          <aside class="notes">
            So, I had two kinds of tests, one was the functional tests, to test
            the API contracts, verify the responses of the API calls given
            valid requests, or malformed requests.

            The other, was system integration tests. These were more complex
            tests, as they were going to be verifying the integration between
            Identity(keystone), Compute, Load balancers and Autoscale. For
            example, When a user created a scaling group, it will verify that
            the minimum number of servers on the group are provisioned
            successfully, as in the servers are active and they exist as nodes
            on the load balancers. Or if a server, went into an error state,
            that can happen, Autoscale was able to re-provision that server.
          </aside>
        </section>

        <section>
          <h1>Testing Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">
            All these tests were running against the real services. Servers
            could take over a minute, ten minutes to provision and the tests
            would run that much longer. Sometimes, the tests would fail due to
            random failures, like a server would go into error state, where as
            the test was expecting it to go into an active state. The tests for
            the negative scenarios, like actually testing how autoscale would
            behave if the server went into an error state, could not be
            tested. This is just not something that could be reproduced.
          </aside>
        </section>

        <section>
          <h1>More Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">
            Well, the test coverage was improving as I continued to add tests,
            oblivious of the time it was taking run the entire test suite! Now,
            we had started using these tests as a gate in out merge
            pipeline. But the tests were running for so long and were sometimes
            flaky. Nobody dared to run these tests locally! Not even me, when I
            was adding more tests! Also, our peers from the compute and load
            balancers teams, whose resources we were using up for our
            "Auto-scale" testing, were _not_ happy! So much, so that, we were
            pretty glad, we were in a remote office!
          </aside>
        </section>

        <section>
          <h1>We've Had Enough!</h1>
          <h2>(on Auto Scale)</h2>
          <aside class="notes">
            This had to change! we needed something! to save us from the slow and flaky
            testing!
          </aside>
        </section>

        <section>
          <h1>What's Good About Fakes?</h1>
          <h2>Split Up This Slide</h2>
          <aside class="notes">
            When you're testing against a real service, you have to deal with
            the whole spectrum of real failures which might come back from that
            service.  This is of course important for validating a product,
            ensuring that it works as expected in a realistic deployment
            environment, even if your colleagues on the compute and load
            balancer teams get mad at you.

            But when you write code to interact with a service, you need to
            handle a wide variety of error cases.

            Your positive-path code - the code that submits a request and gets
            the response that it expects - is going to get lots of testing in
            the real world.  Most interactions with services are successful,
            and the operators of those services always strive to make ever more
            of those interactions successful.  So most likely, the
            positive-path code is going to get exercised all the time and you
            will have plenty of opportunities to flush out bugs.

            Your negative-path code, on the other hand, will only get invoked
            in production when there's a real error.  If everything is going as
            planned, this should be infrequent.

            It's really important to get negative-path code right.  If
            everything is going well, it's probably okay if your code has a
            couple of bugs.  You might be able to manually work around them.

            But if things are starting to fail with some regularity in your
            cloud, that is exactly the time you want to make sure *your* system
            is behaving correctly: accurately reporting the errors, measuring
            the statistics, and allowing you to stay on top of incident
            management for your service.

            When you test against a real service, you are probably testing
            against a staging instance.  And, if your staging instance is
            typical, it probably doesn't have as much hardware, or as many
            concurrent users, as your production environment.  Every additional
            piece of harware or concurrent user is another opportunity for
            failure, so that means your staging environment is probably even
            less likely to fail.

            I've been in the software industry for long enough now to remember
            where this part of the talk would be the hardest part - the part
            where we try to sell the idea that code coverage and automated
            testing is really important.  Luckily we have moved on from the bad
            old days of the 90s, when most teams didn't have build automation,
            and if they wanted it they might not even be able to afford it.

            Luckily today we are all somewhat more enlightened, and we know
            that testing is important and full code coverage is important.  So
            when we write code like this:

            (slide: try/except with code)

            ... we know that we need to write tests for this part:

            (highlight 'except' part)

            ... by writing a mock for it.

            The problem with the traditional definition of a mock is that each
            mock is defined just for the specific test that it's mocking.

            Let's take a specific example from OpenStack Compute.

            In June of this year, OpenStack Compute [introduced a
            bug](https://github.com/openstack/nova/commit/1a6d32b9690b4bff709dc83bcf4c2d3a65fd7c3e)
            making it impossible to revoke a certificate.

            (This is not a criticism of Nova itself; [the bug was later
            fixed](https://github.com/openstack/nova/commit/96b39341d5a6ea91d825d979e2381b9949b26e27).)

            The bug here is that `chdir` does not actually return a value.
            Because these tests construct their own mocks for `chdir`, we
            properly cover all the code, but the code is not integrated with a
            system that is verified in any way against what the real system (in
            this case, Python's chdir) does.

            In this *specific* case, we might have simply tested against a real
            directory structure in the file system, because relative to the
            value of testing against a real implementation, creating a
            directory is not a terribly expensive operation.

            However, an OpenStack cloud is a significantly more complex system
            than `chdir`.  If you are developing an application that depends on
            OpenStack, creating a real cloud to test against is far too
            expensive and slow as Autoscale's experience shows.  Creating a
            one-off mock for every test is fast to get started with and fast to
            run, but is error prone and rapidily becomes a significant
            maintainance burden of its own right. Autoscale needed something
            that was quick to deploy, and lightweight to run like a mock, but
            realistic and usable in an integration scenario like a __real__
            system.

            This is were Mimic comes in.
          </aside>
        </section>

        <section>
          <h1>First Version of Mimic</h1>
          <aside class="notes">
            Mimic was built as a stand-in service for Identity, Compute and
            Rackspace Load balancers, the services that autoscale depends
            upon. Mimic initially implemented only the subset of each of these
            endpoints needed by Autoscale.

            The essence of Mimic is pretending.  The first thing that you must
            do to interact with it is to pretend to authenticate.

            Although Mimic does not validate credentials - all authentications
            will succeed - as with the real Identity endpoint, Mimic's identity
            endpoint has a service catalog which includes URLs for all the
            services it provides.  A well behaved OpenStack client will use the
            service catalog to look up URLs for its service endpoints. Such a
            client will only need two pieces of configuration to begin
            communicating with the cloud, i.e. credentials and the identity
            endpoint. A client written this way will only need to change the
            Identity endpoint to be that of Mimic.

            When you ask Mimic to create a server, it pretends to create
            one. This is not like stubbing with static responses: when Mimic
            pretends to build a server, it remembers the information about that
            server and will tell you about it in the subsequent requests.

            Mimic was originally created to speed things up. So, it was very
            important that it be fast both to respond to requests and to have
            developers setup. So it was built using in-memory data
            structures. Mimic has minimal software dependencies - almost
            entirely pure Python. No service dependencies and no
            configuration. It is entirely self- contained.

            Here is a quick demo.
          </aside>
        </section>

        

        <section>
          <aside class="notes">
            (Demo here: nova python client pretending to create a server, list
            servers, delete server, against mimic.)

            Here is an example of how we could use Mimic against the OpenStack
            nova python client.  We're including the Mimic identity endpoint as
            the `AUTH_URL`.
          </aside>
        </section>

        <section>
          <h1>Using Mimic</h1>
          <h2>(on Auto Scale)</h2>
          <h3>TODO: split up this slide</h3>

          <aside class="notes">
            We did this with Autoscale, and pointed its tests at a Mimic
            instance.  This reduced the test time exponentially. Before Mimic,
            the functional tests would take 15 minutes, and now they run in 30
            seconds. The system integration tests would take 3 *hours* or more,
            cause if one of the servers in the test remained in the "building"
            state for fifteen minutes longer than usual, then the tests ran
            fifteen minutes slower.  That 3 *hours* (or more) went down to be
            less than 3 *minutes* (consistently) to complete!

            Our dev VMs are configured to run tests against Mimic, so
            developers get immediate feedback on the code being written and
            they can work offline without having to worry about uptimes of the
            upstream systems.

            Glyph: But Lekha, what about all the error injection stuff I
            mentioned before?  Does Mimic do that?  How did this set-up test
            Autoscale's error conditions?

            Well Glyph, I am as pleased as I am suprised that you ask that.
            Mimic does simulate errors.

            Earlier, when I said Mimic pretends to create a server, that wasn't
            entirely true - sometimes Mimic pretends to *not* create a server.
            It uses the metadata provided during the creation of the server. It
            inspects the metadata, and sets the state of the server
            respectively. For example, setting a `metadata` of say
            `"server_building": 30`, will make the server stay in building
            state for 30 seconds. Similarly, servers can be made to go into
            error state on creation.

            Autoscale's purpose is to get the user the right number of servers,
            even if a number of attempts to create one failed. We chose to use
            `metadata` for error injection so that requests with injected
            errors could also be run against real services. For Autoscale, the
            expected end result is the same number of servers created,
            irrespective of the number of failures. But this behavior may also
            be useful to many other applications because retrying is a common
            pattern for handling errors.

            However, the first implementation of mimic had some flaws, it was
            fairly Rackspace specific. It implemented only the services
            required by autoscale, and they were all implemented as the
            core. It ran each service on a different port, meaning that for N
            endpoints you would need not just N port numbers, but N
            *consecutive* port numbers. It allowed for testing error scenarios,
            but only using the metadata. This was not useful for all cases, for
            example, for a control panel that does not not allow every UI
            action to contain metadata.

            (TODO: ask someone if Horizon matches this description?)

            Mimic also did not implement multiple regions.  It used global
            variables for storing all state, which meant that it was hard to
            add additional endpoints with different state in the same running
            mimic instance.
          </aside>
        </section>

        <section>
          <h1>Refactoring Mimic</h1>
          <h2>(To Use with things other than Auto Scale)</h2>
          <aside class="notes">
            Mimic had an ambitious vision: to be a one-stop mock for all
            OpenStack and Rackspace services that needed fast integration
            testing.  However, its architecture at the time severely limited
            the ability of other teams to use it or contribute to it.  As Lekha
            mentioned, it was specific not only to Rackspace but to Autoscale.

            On balance, Mimic was also extremely simple.  It followed the You
            Aren't Gonna Need It principle of extreme programming very well,
            and implemented just the bare minimum to satisfy its requirements,
            so there wasn't a whole lot of terrible code to throw out or much
            unnecessary complexity to eliminate.

            There is, however, a corrolary to YAGNI, which is E(ITO)YAGNI:
            Eventually, It Turns Out, You *Are* Going To Need It.  As Mimic
            grew, other services within Rackspace wanted to make use of its
            functionality, and a couple of JSON response dictionaries in global
            variables were not going to cut it any more.

            So we created a plugin API.

            As Lekha mentioned previously, Mimic's Identity endpoint was the
            top-level entry point to Mimic as a service; every other URL was
            available from within the service catalog.  As we were designing
            the plugin API, it was clear that this top-level Identity endpoint
            needed to be the core part of Mimic, and plug-ins would each add an
            entry for themselves to the service catalog.

            Each plugin implements the `IAPIMock` interface, which has only two
            methods: `catalog_entries` and `resource_for_region`.

            `catalog_entries` takes a tenant ID and returns an iterable of
            `Entry` objects, each of which is a name and a collection of
            `Endpoint` objects, each containing a region, a URI version prefix,
            and a tenant ID of its own.

            Put more simply, APIs have catalog entries for each API type, which
            in turn have endpoints for each virtual region they represent.

            `resource_for_region` takes the name of a region, a URI prefix -
            produced by Mimic core to make URI for each service unique - and a
            session store where the API mock may look up state of the resources
            it pretended to provision for the respective
            tenants. `resource_for_region` returns an HTTP resource associated
            with the top level of the given region.  This resource then routes
            this request to any tenant- specific resources associated with the
            full URL path.

            Mimic uses the Twisted plugin system, which has a very simple model
            of how plugins work: you have an abstract interface - some methods
            and attributes that you expect of a plugin - and then plugins
            register themselves by instantiating a provider of that interface
            and placing that instance into a module within a particular
            namespace package.  In Mimic's case, that interface is `IAPIMock`
            and that package is `mimic.plugins`.

            Each service can contribute to the service catalog, provide entries
            and endpoints, and each endpoint gets added within the URI
            hierarchy.

            <!-- TODO: Demo of implementing a plugin! -->

          </aside>
        </section>
        <!-- TODO: insert a segue or transition here! -->
        <section>
          <h1>Error Conditions Repository</h1>
          <aside class="notes">
            Anyone testing a product, will run into unexpected errors. Thats
            why we test!  But we dont know what we dont know, and cant be
            prepared for this ahead of time right!

            When we were running the autoscale tests against Compute, we began
            to see some one-off errors. Like, when provisioning a server, the
            test expected a server to go into a building state for some time
            before it is active, __but__ it would remain in building state for
            over an hour or even would sometimes go into an error state.

            Autoscale had to handle such scenarios gracefully and the code was
            changed to do so. And Mimic provided for a way to tests this
            consistently.

            However, like I said, we dont know what we dont know. We were not
            anticipating any other such errors. But, there were more! And this
            was slow process for us to uncover such errors as we tested against
            the real systems. And we continued to add them to Mimic.

            Now, Wont it be great if not every person writing an application
            that used compute as a dependency had to go through this same cycle
            and have to find all the possible error conditions in a system by
            experience and have to deal with them at the pace that they occur.

            What if we had a repository of all such known errors, that everyone
            contributes to. So the next person using the plugin can use the
            existing ones, and ensure there application behaves consistently
            irrespective of the errors, and be able add any new ones to it.

            Mimic is just that, a repository of all known responses including
            the error responses.
          </aside>
        </section>

        <section>
          <aside class="notes">
            control of time & error injection
          </aside>
        </section>

        <section>
          <h1>Call to Action</h1>
          <aside class="notes">
            Mimic, can be the tool, where you do not have to stand up the
            entire dev stack to understand how an OpenStack API behaves.

            Mimic can be the tool which enables an OpenStack developer to get
            quick feedback on the code he/she is writing and not have to go
            through the gate multiple times to understand that, maybe I should
            have handled that one error, that the upstream system decides to
            throw my way every now and then.
          </aside>
        </section>

        <!--

            <section>
              <aside class="notes">
              </aside>
            </section>

         -->
      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>

    <script src="configuration.js"> </script>
    <script src="custom.js"></script>

  </body>
</html>





            <!--Today we are going to talk about Mimic - an opensource
            framework that allows for testing OpenStack and
            Rackspace APIs. How it is making testing for products in Rackspace
            a cakewalk and how it could potentially do the same for your
            OpenStack-backed applications. Your contributions, will help us get
            there.
          </aside>
        </section> -->