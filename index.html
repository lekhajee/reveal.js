<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Mimic Presentation</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="custom.css">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>

    <!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>Mimic</h1>
          <h2>An API Compatible Mock Service For OpenStack</h2>
        </section>

        <section>
          <h1>Lekha</h1>
          <h3 style="color: gray"> software developer in test</h3>
          <h3 style="color: gray"> (at) Rackspace </h3>
          <aside class="notes">
            Lekha: Hi, I am Lekha Jeevan, a software developer in test at Rackspace
          </aside>
        </section>

        <section>
          <h1>Glyph</h1>
          <h3 style="color: gray"> software developer </h3>
          <h3 style="color: gray"> (at) Rackspace </h3>
          <aside class="notes">
            Glyph: and I'm Glyph, also a software developer at Rackspace
          </aside>
        </section>

        <section>
          <h1>What?</h1>
          <aside class="notes">
            Lekha: Today Glyph and I are here to talk about Mimic. An
            opensource framework that allows for testing of OpenStack and
            Rackpsace APIs.
            <br>
            Mimic has been making testing across products in Rackspace a cake
            walk. And we think it could do the same for the OpenStack- backed
            applications someday. Today, We are not there yet. But your
            contributions will help us get there soon.
          </aside>
        </section>

        <!--section>
          <h1>Sneak Peek</h1>
          <aside class="notes">

           Lekha: Before we even begin to dive into the details, let us take a
           quick sneak peek of how __easy__ it is to get started with Mimic.

          </aside>
        </section>

        <section data-state="first-demo">
          <video src="bootstrap.mp4" id="first-demo-video"></video>
          <aside class="notes">

            Its only a 3 step process: pip install mimic, run mimic and hit
            the endpoint! And... there is our endpoint to be able to
            Authenticate and get a service catalog containing the (OpenStack)
            services that Mimic implements.

          </aside>
        </section-->

        <!--Today we are going to talk about Mimic - an opensource
            framework that allows for testing OpenStack and
            Rackspace APIs. How it is making testing for products in Rackspace
            a cakewalk and how it could potentially do the same for your
            OpenStack-backed applications. Your contributions, will help us get
            there.
          -->

        <section>
          <h1>Why?</h1>
          <aside class="notes">

            Glyph: While Mimic is hopefully general purpose, it was originally
            created for testing a specific application: Rackspace Auto Scale.
            So let's talk about that.

          </aside>
        </section>

        <section>
          <h1>Auto Scale</h1>
          <aside class="notes">

            Lekha: Rackspace Auto Scale is a system which automates the process
            of getting the right amount of compute capacity for an application,
            by creating (scaling up) and deleting (scaling down) servers and
            associating them with load balancers.
            <!-- TO DO: A diagram indicating the above-->

          </aside>
        </section>

        <section>
          <h1>Dependencies</h1>
          <h2>(of Auto Scale)</h2>
          <aside class="notes">
            Lekha: In order to perform these tasks, Auto Scale speaks to three
            back-end Rackspace APIs.
          </aside>
        </section>

        <section>
          <h1>Rackspace Identity</h1>
          <aside class="notes">

            Lekha: Identity for authentication and impersonation.

          </aside>
        </section>

        <section>
          <h1>Rackspace Cloud Servers</h1>
          <aside class="notes">

            Lekha: Cloud Servers for provisioning and deleting servers.

          </aside>
        </section>

        <section>
          <h1>Rackspace Cloud Load Balancers</h1>
          <aside class="notes">

            Lekha: Rackspace Cloud Load Balancers for adding and removing servers to
            load balancers as they are created or deleted.

          </aside>
        </section>

        <section>
          <h3>Rackspace Identity ‚âà OpenStack Identity v2</h3>
          <h3>Rackspace Cloud Servers ‚âà OpenStack Compute</h3>
          <h3>Rackspace Cloud Load Balancers = Custom API</h3>
          <aside class="notes">

            Lekha: Rackspace Identity is API-compatible with OpenStack
            Identity v2. Rackspace Cloud Servers is powered by and API-
            compatible with OpenStack Compute. Although Rackspace Cloud Load
            Balancers is a custom API.

          </aside>
        </section>

        <section>
          <h1>Testing</h1>
          <h2>(for Auto Scale)</h2>
          <aside class="notes">

            Lekha: As Auto Scale was interacting with so many other systems,
            testing Auto Scale did not mean <b>just</b> testing features of Auto
            Scale. But also that, if any of these systems it depended on did not
            behave as expected, Auto Scale did not crumble or crash, but was consistent
            and able to handle these upstream failures gracefully.

            <!-- TO DO: Include diagram to depict this. Something like-->
            <!-- Auto Scale interacting with systems and them not responding.-->
            <!-- But Auto Scale reponding to the end user-->

          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>(for Auto Scale)</h2>
          <aside class="notes">
            Lekha: So, there were two kinds tests for Auto Scale:
          </aside>
        </section>

        <section>
          <h1>Functional</h1>
          <h2>(API contracts)</h2>
          <aside class="notes">

            Lekha: One, was the Functional tests to validate the API
            contracts. These tests verified the responses of the Auto Scale
            API calls given valid, or malformed requests.

          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>System Integration</h2>
          <h2>(Auto Scale -> Identity, Compute and Load Balancers)</h2>
          <aside class="notes">

            Lekha: And the other was the System integration tests. Theese,
            were more complex. These tests verified integration between Auto
            Scale and Identity, Compute, and Load balancers.

          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>System Integration</h2>
          <h2>(Example 1)</h2>

          <aside class="notes">

            Lekha: For example: When a scaling group was created, one such test will verify
            that the servers on that group were provisioned successfully. That
            is, that the servers went into an 'active' state and were added as a nodes to the
            load balancer on the sceling group.

                        <!-- Include diagram to depict this and use a better heading ? -->
          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>System Integration</h2>
          <h2>(Example 2)</h2>

          <aside class="notes">

            Lekha: Or, if a server went into an error state, (yes! that can happen!),
            Auto Scale was able to re-provision that server succefully, and then add that
            active server to the load balancer on the scaling group.

            <!-- Include diagram to depict this and use a better heading ? -->


          </aside>
        </section>

        <section>
          <h1>Testing Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">

            Lekha: All these tests were set up to run against the real services.
            And... here are some observations I had while writing the tests:
            Servers could take over a minute, or ten minutes, or longer to
            provision. And the tests would run that-much-longer.
            <br>
            Sometimes, the tests would fail! due to raandom upstream failures.
            Like a server would go into 'error' state, where as the <b>test</b> was
            expecting it to go into an 'active' state.
            <br>
            And tests for such negative scenarios, like actually testing how
            Auto Scale would behave if the server did go into 'error' state,
            could not be tested. This is something that could <b>not</b> be
            reproduced consistently.

          </aside>
        </section>

        <section>
          <h1>More Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">

            Lekha: Well, the overall test coverage was improving. And I
            continued to add tests, oblivious of the time it was taking run
            the entire test suite!
            <br>
            Later, we had started using these tests as a <em>gate</em>, in our merge
            pipeline.
            <br>
            But, the tests were running for <b>so</b> long and were sometimes flaky.
            Nobody dared to run these tests locally! Not even me, when I was
            adding more tests!
            <br>
            Also, our peers from the compute and load balancers teams, whose
            resources we were using up for our "Auto-scale" testing, were
            <em>not</em> happy! So much, so that, we were <em>pretty</em> glad, we were in a
            remote office!

            <!--TO DO: Pictures to depict each para above-->

          </aside>
        </section>

        <section>
          <h1>We've Had Enough!</h1>
          <h2>(on Auto Scale)</h2>
          <aside class="notes">

            Lekha: But! We had had enough! This had to change! we needed
            something! to save us from these slow flaky tests!

          </aside>
        </section>

        <section>
          <h1>There And Back Again</h1>
          <h2 class="fragment current-visible">Specific ‚Üí General</h2>
          <h2 class="fragment current-visible">General ‚Üí Specific</h2>
          <aside class="notes">
            Glyph: Now that we've had enough, how are we going to solve this
            problem?  Since we've been proceeding from the specific case of
            Auto Scale to the general utility of Mimic, (click) let's go back to
            the general and proceed to the specific.
          </aside>
        </section>

        <section>
          <h1>General ‚Üí</h1>
          <h2 class="fragment visible">Testing For Failure</h2>

          <aside class="notes">
            Glyph: When you have a complex distributed system - that is, a
            service A which makes requests of another service B - you have to
            deal with the whole spectrum of real failures which might come back
            from service B.
          </aside>
        </section>
        <section>
          <h1>Real Services Fail</h1>
          <aside class="notes">
            Glyph: Testing against the real service is of course important for
            validating a product, ensuring that it works as expected in a
            realistic deployment environment, even if your colleagues on the
            compute and load balancer teams get mad at you.
          </aside>
        </section>

        <section>
          <h1>(Unpredictably)</h1>
          <aside class="notes">
            Glyph: But when you write code to interact with a service, you need
            to handle a wide variety of error cases.  Those errors won't come
            back from the real service every time.
          </aside>
        </section>

        <section>
          <h1>Succeeding</h1>
          <h2>At Success</h2>
          <aside class="notes">
            Glyph: Your positive-path code - the code that submits a request
            and gets the response that it expects - is going to get lots of
            testing in the real world.  Most interactions with services are
            successful, and the operators of those services always strive to
            make ever more of those interactions successful.  So most likely,
            the positive-path code is going to get exercised all the time and
            you will have plenty of opportunities to flush out bugs.
          </aside>
        </section>
        <section>
          <h1>Means Failing</h1>
          <h2>At Failure</h2>
          <aside class="notes">
            If you test against real services, your negative-path code, will
            only get invoked in production when there's a real error.  If
            everything is going as planned, this should be infrequent.
          </aside>
        </section>
        <section>
          <h1>Succeeding</h1>
          <h2>At Failure</h2>
          <aside class="notes">
            It's really important to get negative-path code right.  If
            everything is going well, it's probably okay if your code has a
            couple of bugs.  You might be able to manually work around them.
          </aside>
        </section>
        <section>
          <h1 style="font-size: 800%">üòà ‚òÅ</h1>
          <h3>(Production)</h3>
          <aside class="notes">
            But if things are starting to fail with some regularity in your
            cloud - that is to say - if you are using a cloud - that is exactly
            the time you want to make sure <em>your</em> system is behaving
            correctly: accurately reporting the errors, measuring the
            statistics, and allowing you to stay on top of incident management
            for your service.
          </aside>
        </section>
        <section>
          <h1 style="font-size: 800%">üòá ‚òÅ</h1>
          <h3>(Staging)</h3>
          <aside class="notes">
            When you test against a real service, you are probably testing
            against a staging instance.  And, if your staging instance is
            typical, it probably doesn't have as much hardware, or as many
            concurrent users, as your production environment.  Every additional
            piece of harware or concurrent user is another opportunity for
            failure, so that means your staging environment is probably even
            less likely to fail.
          </aside>
        </section>
        <section>
          <pre style="border: 1px solid white; font-size: 70px;"><code class="language-python">import unittest</code></pre>
          <aside class="notes">
            I've been in the software industry for long enough now to remember
            where this part of the talk would be the hardest part - the part
            where we try to sell the idea that code coverage and automated
            testing is really important.  Luckily we have moved on from the bad
            old days of the 90s, when most teams didn't have build automation,
            and if they wanted it they might not even be able to afford it.
          </aside>
        </section>
        <section>
          <h2 style="text-transform: none;"><code>test_stuff ... <span style="color: rgb(0, 255, 0);">[OK]</span></code></h2>
          <aside class="notes">
            Luckily today we are all somewhat more enlightened, and we know
            that testing is important and full code coverage is important.  So
            when we write code like this:
          </aside>
        </section>
        <section>
          <pre style="font-size: 40px"><code class="language-python" style="padding: 1em;">try:
    result = service_request()
except:
    return error
else:
    return ok(result)</code></pre>
          <aside class="notes">
            ... we know that we need to write tests for this part:
          </aside>
        </section>
        <section>
<pre style="font-size: 40px"><span class="codelike" style="padding: 1em;"><span class="keyword">try</span>:
    result = service_request()
<div class="highlighted"><span class="keyword">except</span>:
    <span class="keyword">return</span> error
</div><span class="keyword">else</span>:
    <span class="keyword">return</span> ok(result)</span></pre>
          <aside class="notes">
            (highlight 'except' part)

            ... and one popular way to get test coverage for those error lines
            is by writing a custom mock for it.
          </aside>
        </section>
        <section>
          <img src="Alice_par_John_Tenniel_34_Cropped.png">
          <aside class="notes">
            The problem with the traditional definition of a mock is that each
            mock is defined just for the specific test that it's mocking.
          </aside>
        </section>
        <section>
          <img src="Alice_par_John_Tenniel_34_Cropped_Tears.png">
          <aside class="notes">
            The problem with the traditional definition of a mock is that each
            mock is defined just for the specific test that it's mocking.
            There's a reason that Alice in Wonderland's Mock Turtle is crying.
            Mocking is often a sad enterprise.
            <p>
              Let's take a specific example from OpenStack Compute.
            </p>
          </aside>
        </section>
        <section>
          <pre style="font-size: 24pt;"><code class="language-python">if not os.chdir(ca_folder(project_id)):
    raise exception.ProjectNotFound(
        project_id=project_id)</code></pre>
          <aside class="notes">
            In June of this year, OpenStack Compute [introduced a
            bug](https://github.com/openstack/nova/commit/1a6d32b9690b4bff709dc83bcf4c2d3a65fd7c3e)
            making it impossible to revoke a certificate.  The lines of code at
            fault were these two additions here.

            (This is not a criticism of Nova itself; [the bug was later
            fixed](https://github.com/openstack/nova/commit/96b39341d5a6ea91d825d979e2381b9949b26e27).)

          </aside>
        </section>
        <section>
          <pre style="font-size: 24pt;"><span class="codelike language-python"><span class="highlighted"><span class="keyword">if</span> <span class="keyword">not</span> os.chdir</span>(ca_folder(project_id)):
    <span class="keyword">raise</span> exception.ProjectNotFound(
        project_id=project_id)</span></pre>
          <aside class="notes">
          </aside>
        </section>
        <section>
          <pre style="font-size: 17pt"><code class="language-python">@mock.patch.object(os, 'chdir', return_value=True)
def test_revoke_cert_process_execution_error(self):
    "..."

@mock.patch.object(os, 'chdir', return_value=False)
def test_revoke_cert_project_not_found_chdir_fails(self):
    "..."</code></pre>
          <aside class="notes">
            The bug here is that `chdir` does not actually return a value.
            Because these tests construct their own mocks for `chdir`, we
            properly cover all the code, but the code is not integrated with a
            system that is verified in any way against what the real system (in
            this case, Python's chdir) does.
          </aside>
        </section>
        <section>
          <img src="folders.png" />
          <aside class="notes">
            In this <em>specific</em> case, we might have simply tested against
            a real directory structure in the file system, because relative to
            the value of testing against a real implementation, creating a
            directory is not a terribly expensive operation.
          </aside>
        </section>
        <section>
          <img src="openstack_havana_conceptual_arch.png" />
          <aside class="notes">
            However, an OpenStack cloud is a significantly more complex system
            than `chdir`.  If you are developing an application that depends on
            OpenStack, creating a real cloud to test against is far too
            expensive and slow as Auto Scale's experience shows.  Creating a
            one-off mock for every test is fast to get started with and fast to
            run, but is error prone and rapidily becomes a significant
            maintainance burden of its own right. Auto Scale needed something
            that was quick to deploy, and lightweight to run like a mock, but
            realistic and usable in an integration scenario like a __real__
            system.
          </aside>
        </section>
        <section>
          <h1>Mimic</h1>
          <aside class="notes">
            This is were Mimic comes in.
          </aside>
        </section>
        <section>
          <h1>Mimic</h1>
          <h2>Version 0.0</h2>
          <aside class="notes">
            The first version of Mimic was built as a stand-in service for
            Identity, Compute and Rackspace Load balancers, the services that
            Auto Scale depends on.
          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2 style="visibility: hidden">...</h2>
          <aside class="notes">
            The essence of Mimic is pretending. The first thing that you must
            do to interact with it is to...
          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2>to authenticate</h2>
          <aside class="notes">

            ...pretend to authenticate.
            <br>

            Mimic does not validate credentials - all authentications will
            succeed. As with the real Identity endpoint, Mimic's identity
            endpoint has a service catalog which includes URLs for all the
            services implemented within Mimic. <br>

            A well behaved OpenStack client will use the service catalog to
            look up URLs for its service endpoints. Such a client will only
            need two pieces of configuration to begin communicating with the
            cloud, i.e. credentials and the identity endpoint. A client
            written this way will only need to change the Identity endpoint to
            be that of Mimic.

          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2>to Boot Servers</h2>
          <aside class="notes">

            When you ask Mimic to create a server, it pretends to create one.
            This is not like stubbing with static responses: when Mimic
            pretends to build a server, it remembers the information about
            that server and will tell you about it in the subsequent requests.

          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2>is <em>faster</em> than working</h2>
          <aside class="notes">

            Mimic was originally created to speed things up. So, it was very
            important that it be fast both to respond to requests and to have
            developers setup.

          </aside>
        </section>

        <section>
          <h1>in-memory data structures</h1>
          <aside class="notes">
            It was built using in-memory data structures.
          </aside>
        </section>

        <section>
          <h1>minimal software dependencies</h1>
          <h2>almost entirely pure Python</h2>
          <aside class="notes">
            with minimal software dependencies, almost entirely pure Python.
          </aside>
        </section>

        <section>
          <h1><del>service dependencies</del></h1>
          <aside class="notes">
            No service dependencies
          </aside>
        </section>

        <section>
          <h1><del>configuration</del></h1>
          <aside class="notes">
            No configuration
          </aside>
        </section>

        <section>
          <h1>self- contained</h1>
          <aside class="notes">
            And entirely self-contained.
          </aside>
        </section>

        <section>
          <h1>Demo!</h1>
          <aside class="notes">
            Here is a quick demo.
          </aside>
        </section>
        <section>
          <h1>INSERT DEMO HERE</h1>
          <h2>TODO: Nova Python client pretending to</h2>
          <h3>Create A Server</h3>
          <h3>List Servers</h3>
          <h3>Delete Servers</h3>
          <aside class="notes">
            Here is an example of how we could use Mimic against the OpenStack
            nova python client.  We're including the Mimic identity endpoint as
            the `AUTH_URL`.
          </aside>
        </section>

        <section>
          <h1>Using Mimic</h1>
          <h2>(on Auto Scale)</h2>
          <aside class="notes">

            We did the same thing with Auto Scale. We pointed the tests and
            Autoscale to run against an instance of Mimic.

          </aside>
        </section>


        <section>
          <h1>The Results!</h1>
          <h2>(Functional tests using Mimic)</h2>
          <aside class="notes">

            This reduced the test time exponentially! Before Mimic the functional tests would take...

          </aside>
        </section>

        <section>
          <h1>15 minutes vs. less than 30 seconds</h1>
          <h2>(Functional tests using Mimic)</h2>
          <aside class="notes">

            15 minutes to complete, and now they run in less than 30 seconds!

          </aside>
        </section>

        <section>
          <h1>The Results!</h1>
          <h2>(System Integration tests using Mimic)</h2>
          <aside class="notes">

            The system integration tests would take 3 *hours* or more, cause
            if one of the servers in the test remained in the "building" state
            for fifteen minutes longer than usual, then the tests ran fifteen
            minutes slower.

          </aside>
        </section>

        <section>
          <h3>Functional tests:</h3>
          <h1>3 hours or more</h1>
          <h3>against a real system</h3>
          <h2>vs.</h2>
          <h1>3 minutes</h1>
          <h3>against Mimic</h3>
          <aside class="notes">

            This went down to be less than 3 *minutes* (consistently) to
            complete!

          </aside>
        </section>

        <section>
          <h1>Developing in<br>Airplane Mode!</h1>
          <aside class="notes">

            Our dev VMs are now configured to run against Mimic.

            One of our devs from the Rackspace Cloud Intelligence team, calls
            this "Developing on Airplane Mode!", as devs get immediate
            feedback on the code being written and we can work offline without
            having to worry about uptimes of the upstream systems.

          </aside>
        </section>
        <section>
          <h1> What about<br>error injection? </h1>
          <aside class="notes">
            Glyph: But Lekha, what about all the error injection stuff I
            mentioned before?  Does Mimic do that?  How did this set-up test
            Auto Scale's error conditions?

          </aside>
        </section>

        <section>
          <h1> Mimic does<br>simulate errors </h1>
          <aside class="notes">

            Well Glyph, I am as pleased as I am suprised that you ask that.
            Mimic does simulate errors.

          </aside>
        </section>

        <section>
          <h1>Error injection using <code>metadata</code></h1>
          <aside class="notes">

            Earlier, when I said Mimic pretends to create a server, that wasn't
            entirely true - sometimes Mimic pretends to *not* create a server.
            It uses the metadata provided during the creation of the server,
            inspects the metadata, and sets the state of the server
            respectively. 

          </aside>
        </section>

        <section data-state="server-in-building-and-error-state-demo">
          <video src="server-building-and-error-state.mp4" id="server-error-injection-video"></video>
          <aside class="notes">

            Lets go back to the demo and see how this can be done. We are
            creating a server with `metadata`: `"server_building": 30`.
            This... will make the server stay in building state for 30
            seconds. Similarly, servers can also be made to go into error
            state on creation, using the `metadata`: `"server_error": True`.

          </aside>
        </section>


        <section>
          <h1>Retry On Errors</h1>
          <aside class="notes">

            For the purposes of Auto Scale it was important that we have
            right number of servers on a scaling group, even if a number of
            attempts to create one failed. We chose to use `metadata` for
            error injection so that requests with injected errors could also
            be run against real services. For Auto Scale, the expected end
            result is the same number of servers created, irrespective of the
            number of failures. But this behavior may also be useful to many
            other applications because retrying is a common pattern for
            handling errors.

          </aside>
        </section>

        <section>
          <h2>Mimic 0.0 was...</h2>
          <h1>Too Limited</h1>
          <aside class="notes">

            However, the first implementation of mimic had some flaws, it was
            fairly Rackspace specific and only implemented the endpoints of
            the services that Autoscale depends upon . And they were all
            implemented as part of Mimic's core. It ran each service on a
            different port, meaning that for N endpoints you would need not
            just N port numbers, but N *consecutive* port numbers. It allowed
            for testing error scenarios, but only using the metadata. This was
            not useful for all cases, for example, for a control panel that
            does not not allow every UI action to contain metadata.  <!--
            (TODO: ask someone if Horizon matches this description?) -->

          </aside>
        </section>
        <section>
          <h2>Mimic 0.0 was...</h2>
          <h1>Single Region</h1>
          <aside class="notes">
            Mimic also did not implement multiple regions.  It used global
            variables for storing all state, which meant that it was hard to
            add additional endpoints with different state in the same running
            mimic instance.
          </aside>
        </section>
        <section>
          <h2>Beyond Auto Scale:</h2>
          <h1>Refactoring Mimic</h1>
          <aside class="notes">
            Mimic had an ambitious vision: to be a one-stop mock for all
            OpenStack and Rackspace services that needed fast integration
            testing.  However, its architecture at the time severely limited
            the ability of other teams to use it or contribute to it.  As Lekha
            mentioned, it was specific not only to Rackspace but to Auto Scale.

          </aside>
        </section>
        <section>
          <h1>YAGNI</h1>
          <aside class="notes">
            On balance, Mimic was also extremely simple.  It followed the You
            Aren't Gonna Need It principle of extreme programming very well,
            and implemented just the bare minimum to satisfy its requirements,
            so there wasn't a whole lot of terrible code to throw out or much
            unnecessary complexity to eliminate.
          </aside>
        </section>
        <section>
          <h1>E(ITO)YAGNI</h1>
          <aside class="notes">
            There is, however, a corrolary to YAGNI, which is E(ITO)YAGNI:
            Eventually, It Turns Out, You *Are* Going To Need It.  As Mimic
            grew, other services within Rackspace wanted to make use of its
            functionality, and a couple of JSON response dictionaries in global
            variables were not going to cut it any more.
          </aside>
        </section>
        <section>
          <h1>Plugins!</h1>
          <aside class="notes">
            So we created a plugin API.
          </aside>
        </section>
        <section>
          <h1>Identity</h1>
          <h2>Is the Entry Point</h2>
          <h3>(Not A Plugin)</h3>
          <aside class="notes">
            As Lekha mentioned previously, Mimic's Identity endpoint was the
            top-level entry point to Mimic as a service; every other URL was
            available from within the service catalog.  As we were designing
            the plugin API, it was clear that this top-level Identity endpoint
            needed to be the core part of Mimic, and plug-ins would each add an
            entry for themselves to the service catalog.
          </aside>
        </section>
        <section>
          <h1><code>IAPIMock</code></h1>
          <h2>Plugin Interface</h2>

          <aside class="notes">
            Each plugin implements the `IAPIMock` interface, which has only two
            methods:
          </aside>
        </section>
        <section>
          <h2><code>class IAPIMock():</code></h2>
          <h3 class="fragment"><code>catalog_entries(...)</code></h3>
          <h3 class="fragment"><code>resource_for_region(...)</code></h3>
          <br />
          <h3 class="fragment">(that's it!)</h3>

          <aside class="notes">
            `catalog_entries` and `resource_for_region`.

            `catalog_entries` takes a tenant ID and returns an iterable of
            `Entry` objects, each of which is a name and a collection of
            `Endpoint` objects, each containing a region, a URI version prefix,
            and a tenant ID of its own.

            Put more simply, APIs have catalog entries for each API type, which
            in turn have endpoints for each virtual region they represent.
          </aside>
        </section>
        <section>
          <aside class="notes">

            `resource_for_region` takes the name of a region, a URI prefix -
            produced by Mimic core to make URI for each service unique - and a
            session store where the API mock may look up state of the resources
            it pretended to provision for the respective
            tenants. `resource_for_region` returns an HTTP resource associated
            with the top level of the given region.  This resource then routes
            this request to any tenant- specific resources associated with the
            full URL path.

          </aside>
        </section>
        <section>
          <aside class="notes">
            Mimic uses the Twisted plugin system, which has a very simple model
            of how plugins work: you have an abstract interface - some methods
            and attributes that you expect of a plugin - and then plugins
            register themselves by instantiating a provider of that interface
            and placing that instance into a module within a particular
            namespace package.  In Mimic's case, that interface is `IAPIMock`
            and that package is `mimic.plugins`.

          </aside>
        </section>
        <section>
          <aside class="notes">
            Each service can contribute to the service catalog, provide entries
            and endpoints, and each endpoint gets added within the URI
            hierarchy.

            <!-- TODO: Demo of implementing a plugin! -->

          </aside>
        </section>
        <!-- TODO: insert a segue or transition here! -->
        <section>
          <h1>Error Conditions Repository</h1>
          <aside class="notes">
            Anyone testing a product, will run into unexpected errors. Thats
            why we test!  But we dont know what we dont know, and cant be
            prepared for this ahead of time right!
          </aside>
        </section>
        <section>
          <h1>Discovering Errors</h1>
          <h2>Against Real Services</h2>
          <aside class="notes">
            When we were running the Auto Scale tests against Compute, we began
            to see some one-off errors. Like, when provisioning a server, the
            test expected a server to go into a building state for some time
            before it is active, __but__ it would remain in building state for
            over an hour or even would sometimes go into an error state.
          </aside>
        </section>
        <section>
          <h1>Record Those Errors</h1>
          <h2>Within Mimic</h2>
          <aside class="notes">
            Auto Scale had to handle such scenarios gracefully and the code was
            changed to do so. And Mimic provided for a way to tests this
            consistently.
          </aside>
        </section>
        <section>
          <h1>Discover More Errors</h1>
          <h2>Against Real Services</h2>
          <aside class="notes">
            However, like I said, we dont know what we dont know. We were not
            anticipating any other such errors. But, there were more! And this
            was slow process for us to uncover such errors as we tested against
            the real systems. And we continued to add them to Mimic.
          </aside>
        </section>
        <section>
          <h1>Record Those Errors</h1>
          <h2>For The Next Project</h2>
          <aside class="notes">
            Now, Wont it be great if not every person writing an application
            that used compute as a dependency had to go through this same cycle
            and have to find all the possible error conditions in a system by
            experience and have to deal with them at the pace that they occur.
          </aside>
        </section>
        <section>
          <h1>Share A Repository</h1>
          <h2>For Future Projects</h2>
          <aside class="notes">
            What if we had a repository of all such known errors, that everyone
            contributes to. So the next person using the plugin can use the
            existing ones, and ensure there application behaves consistently
            irrespective of the errors, and be able add any new ones to it.
          </aside>
        </section>
        <section>
          <h1>Mimic Is A Repository</h1>
          <aside class="notes">
            Lekha: Mimic is just that, a repository of all known responses
            including the error responses.
          </aside>
        </section>

        <section>
          <h1>Control</h1>
          <aside class="notes">
            Glyph: In addition to storing a repository of errors, Mimic allows
            for finer control of behavior beyond simple success and error.
          </aside>
        </section>

        <section>
          <h1>Call to Action</h1>
          <aside class="notes">
            Mimic, can be the tool, where you do not have to stand up the
            entire dev stack to understand how an OpenStack API behaves.

            Mimic can be the tool which enables an OpenStack developer to get
            quick feedback on the code he/she is writing and not have to go
            through the gate multiple times to understand that, maybe I should
            have handled that one error, that the upstream system decides to
            throw my way every now and then.
          </aside>
        </section>

        <!--

            <section>
              <aside class="notes">
              </aside>
            </section>

            -->

      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>

    <script src="configuration.js"> </script>
    <script src="custom.js"></script>

  </body>
</html>
